import torch
import sys
from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoConfig

# CRITICAL: Patch PEFT before using it to handle Phi-4's missing method
from peft import peft_model, LoraConfig, get_peft_model

_original_peft_init = peft_model.PeftModelForCausalLM.__init__

def _patched_peft_init(self, model, peft_config, adapter_name="default", **kwargs):
    """
    Patched PEFT init that adds prepare_inputs_for_generation if missing.
    This works around Phi-4-multimodal's architectural bug.
    """
    # Add missing method if needed
    if not hasattr(model, 'prepare_inputs_for_generation'):
        def prepare_inputs_for_generation(*args, **kwargs):
            return {}
        model.prepare_inputs_for_generation = prepare_inputs_for_generation

    # Call original init
    _original_peft_init(self, model, peft_config, adapter_name, **kwargs)

# Apply the patch
peft_model.PeftModelForCausalLM.__init__ = _patched_peft_init
print("âœ… Patched PEFT to handle Phi-4's missing prepare_inputs_for_generation method")

def get_model_and_processor(
    model_id: str = "microsoft/Phi-4-multimodal-instruct",
    lora_rank: int = 64,
    lora_alpha: int = 128
):
    # 1. ä¸ä½¿ç”¨é‡åŒ–ï¼ˆé¸é … Bï¼šå®Œæ•´ç²¾åº¦è¨“ç·´ï¼‰
    # æ³¨æ„ï¼šPhi-4 çš„å…§å»º LoRA åœ¨é‡åŒ–æ™‚ä¹Ÿæœƒè¢«é‡åŒ–ï¼Œå°è‡´ç„¡æ³•è¨“ç·´
    # è§£æ±ºæ–¹æ¡ˆï¼šä¸ä½¿ç”¨é‡åŒ–ï¼Œç›´æ¥ä»¥ bfloat16 åŠ è¼‰
    # VRAM éœ€æ±‚ï¼šç´„ 26-30GBï¼ˆéœ€è¦è¶³å¤ çš„ GPU è¨˜æ†¶é«”ï¼‰
    bnb_config = None  # ä¸ä½¿ç”¨é‡åŒ–

    # model_id = "microsoft/Phi-4-multimodal-instruct"
    model_path = "/Users/xrickliao/WorkSpaces/LLM_Repo/models/Phi-4-multimodal-instruct"

    # 2. åŠ è¼‰è™•ç†å™¨
    processor = AutoProcessor.from_pretrained(model_path,
                                            local_files_only=True,
                                            trust_remote_code=True)

    # 3. åŠ è¼‰ä¸¦ä¿®æ”¹é…ç½®
    # trust_remote_code=True æœƒåœ¨é€™è£¡è¼‰å…¥ modeling_phi4mm æ¨¡çµ„
    config = AutoConfig.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    # 4. PEFT/LoRA å…¼å®¹æ€§å•é¡Œçš„å˜—è©¦ä¿®è£œ (å·²çŸ¥ç„¡æ•ˆ)
    # Phi-4-multimodal å…§å»º LoRA å¯¦ä½œèˆ‡ PEFT ä¸å…¼å®¹
    # trust_remote_code æœƒé‡æ–°è¼‰å…¥æ¨¡çµ„ï¼Œå°è‡´ä»»ä½• monkey-patch å¤±æ•ˆ
    # è©³è¦‹: claudedocs/peft_lora_incompatibility.md
    #
    # æˆ‘å€‘ä¾è³´æ¨¡å‹çš„å…§å»º LoRA ç³»çµ±ï¼Œé€é config.speech_lora å’Œ config.vision_lora é…ç½®
    # ï¼ˆå·²åœ¨ä¸‹æ–¹æ­¥é©Ÿ 4b å®Œæˆé…ç½®ï¼‰

    # 4a. ç¦ç”¨ Flash Attention 2ï¼ˆApple Silicon ä¸æ”¯æŒï¼‰
    config._attn_implementation = "eager"

    # 4b. ä¿®æ”¹å…§å»º LoRA é…ç½®ä»¥ç¬¦åˆè«–æ–‡åƒæ•¸
    # Phi-4-multimodal å¼·åˆ¶è¦æ±‚ vision_lora å’Œ speech_lora å­˜åœ¨
    # æˆ‘å€‘èª¿æ•´åƒæ•¸ä»¥ç¬¦åˆè«–æ–‡è¦æ ¼ï¼ˆr=64, alpha=128ï¼‰
    # æ³¨æ„ï¼šæ¨¡å‹æœƒè‡ªå‹•æ‡‰ç”¨é€™äº› LoRAï¼Œç„¡éœ€å¤–éƒ¨ PEFT

    # Speech LoRA: ä½¿ç”¨é è¨“ç·´æ¨¡å‹çš„åŸå§‹é…ç½®
    # æ³¨æ„ï¼šé è¨“ç·´æ¨¡å‹å·²åŒ…å« LoRA æ¬Šé‡ï¼Œå¿…é ˆä½¿ç”¨ç›¸åŒçš„ rank
    # åŸå§‹é…ç½®ï¼šr=320, alpha=640ï¼ˆèˆ‡è«–æ–‡ä¸åŒï¼ï¼‰
    config.speech_lora = {
        'r': 320,                 # é è¨“ç·´æ¨¡å‹: 320ï¼ˆä¸æ˜¯è«–æ–‡çš„ 64ï¼‰
        'lora_alpha': 640,        # é è¨“ç·´æ¨¡å‹: 640ï¼ˆä¸æ˜¯è«–æ–‡çš„ 128ï¼‰
        'layer': '((layers.*self_attn\\.(qkv|o)_proj)|(layers.*mlp\\.(gate_up|down)_proj))',
        'dp': 0.01                # é è¨“ç·´æ¨¡å‹: 0.01
    }

    # Vision LoRA: ä½¿ç”¨é è¨“ç·´æ¨¡å‹çš„åŸå§‹é…ç½®
    config.vision_lora = {
        'r': 256,                 # é è¨“ç·´æ¨¡å‹: 256
        'lora_alpha': 512,        # é è¨“ç·´æ¨¡å‹: 512
        'layer': 'layers.*((self_attn\\.(qkv_proj|o_proj))|(mlp\\.(gate_up|down)_proj))',
        'dp': 0.0                 # é è¨“ç·´æ¨¡å‹: 0.0
    }

    # 5. ä½¿ç”¨ä¿®æ”¹å¾Œçš„é…ç½®åŠ è¼‰æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        config=config,
        local_files_only=True,
        quantization_config=bnb_config,  # None = ä¸ä½¿ç”¨é‡åŒ–
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,  # ä½¿ç”¨ bfloat16 ç²¾åº¦
    )

    # 6. å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ä»¥æé«˜è¨˜æ†¶é«”æ•ˆç‡
    model.gradient_checkpointing_enable()
    # æ³¨æ„ï¼šä¸ä½¿ç”¨é‡åŒ–æ™‚ï¼Œä¸éœ€è¦ prepare_model_for_kbit_training

    # 7. LoRA å·²ç”±æ¨¡å‹å…§å»ºæ©Ÿåˆ¶è‡ªå‹•æ‡‰ç”¨
    # Phi-4-multimodal åœ¨ __init__ ä¸­å·²æ‡‰ç”¨ vision_lora å’Œ speech_lora
    # ç„¡éœ€é¡å¤–çš„ get_peft_model() èª¿ç”¨
    # æˆ‘å€‘åœ¨æ­¥é©Ÿ 3b å·²è¨­å®šç¬¦åˆè«–æ–‡çš„ LoRA åƒæ•¸

    # ä¿å­˜ LoRA é…ç½®ä¾›åƒè€ƒï¼ˆé›–ç„¶ä¸ä½¿ç”¨ PEFT ç›´æ¥æ‡‰ç”¨ï¼‰
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules="all-linear",
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        modules_to_save=["embed_tokens", "lm_head"]
    )

    # 8. æª¢æŸ¥ä¸¦å ±å‘Š LoRA åƒæ•¸ç‹€æ…‹
    #
    # âš ï¸ å·²çŸ¥é™åˆ¶ï¼šPhi-4 å…§å»º LoRA èˆ‡ QLoRA 4-bit é‡åŒ–ä¸å…¼å®¹
    #
    # å•é¡Œï¼š
    # - Phi-4 çš„å…§å»º LoRA åƒæ•¸åœ¨æ¨¡å‹é‡åŒ–æ™‚ä¹Ÿè¢«é‡åŒ–ç‚º uint8
    # - é‡åŒ–åƒæ•¸ç„¡æ³•è¨­ç½® requires_grad = Trueï¼ˆæœƒå¼•ç™¼ RuntimeErrorï¼‰
    # - prepare_model_for_kbit_training() ç„¡æ³•è‡ªå‹•è™•ç† Phi-4 çš„å…§å»º LoRA
    #
    # æš«æ™‚è§£æ±ºæ–¹æ¡ˆï¼š
    # - æ¨¡å‹æˆåŠŸåŠ è¼‰ï¼Œæ‰€æœ‰åŸºç¤åŠŸèƒ½æ­£å¸¸
    # - LoRA åƒæ•¸å­˜åœ¨ä½†ç„¡æ³•è¨“ç·´ï¼ˆrequires_grad = Falseï¼‰
    # - å¯ä»¥é€²è¡Œæ¨ç†ï¼Œä½†ç„¡æ³•é€²è¡Œ LoRA å¾®èª¿
    #
    # å®Œæ•´è§£æ±ºæ–¹æ¡ˆï¼ˆéœ€è¦é€²ä¸€æ­¥å¯¦ä½œï¼‰ï¼š
    # 1. é¸é … A: ä¸ä½¿ç”¨ load_in_4bitï¼Œæ”¹ç”¨ load_in_8bit æˆ– fp16
    # 2. é¸é … B: ä¿®æ”¹æ¨¡å‹åŠ è¼‰é‚è¼¯ï¼Œé¸æ“‡æ€§é‡åŒ–ï¼ˆLLM 4-bitï¼ŒLoRA åƒæ•¸ fp16ï¼‰
    # 3. é¸é … C: å®Œå…¨ç¦ç”¨å…§å»º LoRAï¼Œæ”¹ç”¨å¤–éƒ¨ PEFTï¼ˆéœ€ä¿®æ”¹æ¨¡å‹åŸå§‹ç¢¼ï¼‰
    #
    # åƒè€ƒ: claudedocs/peft_lora_incompatibility.md

    # çµ±è¨ˆåƒæ•¸ç‹€æ…‹
    lora_params = [(name, p) for name, p in model.named_parameters() if "lora" in name.lower()]
    trainable_lora = sum(1 for _, p in lora_params if p.requires_grad)
    total_lora = len(lora_params)

    all_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())

    print(f"\nğŸ“Š åƒæ•¸çµ±è¨ˆ:")
    print(f"  ç¸½åƒæ•¸: {all_params:,}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {all_trainable:,} ({100*all_trainable/all_params:.4f}%)")
    print(f"  LoRA å±¤æ•¸: {total_lora}")
    print(f"  å¯è¨“ç·´ LoRA å±¤: {trainable_lora}")

    if trainable_lora == 0 and total_lora > 0:
        print(f"\nâš ï¸  è­¦å‘Š: ç™¼ç¾ {total_lora} å€‹ LoRA åƒæ•¸å±¤ï¼Œä½†å…¨éƒ¨è¢«å‡çµï¼ˆquantized uint8ï¼‰")
        print(f"   æ¨¡å‹å¯ç”¨æ–¼æ¨ç†ï¼Œä½†ç„¡æ³•é€²è¡Œ LoRA å¾®èª¿è¨“ç·´")
        print(f"   è©³è¦‹: claudedocs/peft_lora_incompatibility.md")

    return model, processor, peft_config