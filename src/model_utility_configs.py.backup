"""
é›™ LoRA é…ç½®æ¨¡å‹è¼‰å…¥å·¥å…·

æä¾›å…©ç¨®é…ç½®ï¼š
1. é è¨“ç·´é…ç½®ï¼ˆr=320ï¼‰ï¼šä½¿ç”¨é è¨“ç·´ LoRA æ¬Šé‡
2. è«–æ–‡é…ç½®ï¼ˆr=64ï¼‰ï¼šå¾é›¶è¨“ç·´ï¼Œåš´æ ¼å¾©ç¾è«–æ–‡
"""

import torch
import sys
from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoConfig
from peft import peft_model, LoraConfig, get_peft_model

# CRITICAL: Patch PEFT before using it to handle Phi-4's missing method
_original_peft_init = peft_model.PeftModelForCausalLM.__init__

def _patched_peft_init(self, model, peft_config, adapter_name="default", **kwargs):
    """
    Patched PEFT init that adds prepare_inputs_for_generation if missing.
    This works around Phi-4-multimodal's architectural bug.
    """
    if not hasattr(model, 'prepare_inputs_for_generation'):
        def prepare_inputs_for_generation(*args, **kwargs):
            return {}
        model.prepare_inputs_for_generation = prepare_inputs_for_generation

    _original_peft_init(self, model, peft_config, adapter_name, **kwargs)

peft_model.PeftModelForCausalLM.__init__ = _patched_peft_init
print("âœ… Patched PEFT to handle Phi-4's missing prepare_inputs_for_generation method")


def get_model_and_processor_pretrained(
    model_id: str = "microsoft/Phi-4-multimodal-instruct"
):
    """
    é…ç½® 1ï¼šé è¨“ç·´ LoRA é…ç½®ï¼ˆr=320ï¼‰

    - Speech LoRA: r=320, alpha=640, dropout=0.01
    - Vision LoRA: r=256, alpha=512, dropout=0.0
    - å¯è¨“ç·´åƒæ•¸: 830M (14.9%)
    - å„ªé»: å¾é è¨“ç·´ LoRA é–‹å§‹ï¼Œæ”¶æ–‚æ›´å¿«
    - è¼¸å‡ºç›®éŒ„: output/pretrained_r320/
    """

    bnb_config = None  # bfloat16ï¼Œç„¡é‡åŒ–
    model_path = "/Users/xrickliao/WorkSpaces/LLM_Repo/models/Phi-4-multimodal-instruct"

    processor = AutoProcessor.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config = AutoConfig.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config._attn_implementation = "eager"

    # ä½¿ç”¨é è¨“ç·´æ¨¡å‹çš„åŸå§‹ LoRA é…ç½®
    config.speech_lora = {
        'r': 320,
        'lora_alpha': 640,
        'layer': '((layers.*self_attn\\.(qkv|o)_proj)|(layers.*mlp\\.(gate_up|down)_proj))',
        'dp': 0.01
    }

    config.vision_lora = {
        'r': 256,
        'lora_alpha': 512,
        'layer': 'layers.*((self_attn\\.(qkv_proj|o_proj))|(mlp\\.(gate_up|down)_proj))',
        'dp': 0.0
    }

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        config=config,
        local_files_only=True,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )

    model.gradient_checkpointing_enable()

    peft_config = LoraConfig(
        r=320,
        lora_alpha=640,
        target_modules="all-linear",
        lora_dropout=0.01,
        bias="none",
        task_type="CAUSAL_LM",
    )

    # çµ±è¨ˆåƒæ•¸
    lora_params = [(name, p) for name, p in model.named_parameters() if "lora" in name.lower()]
    trainable_lora = sum(1 for _, p in lora_params if p.requires_grad)
    total_lora = len(lora_params)
    all_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())

    print(f"\nğŸ“Š ã€é è¨“ç·´é…ç½® r=320ã€‘åƒæ•¸çµ±è¨ˆ:")
    print(f"  ç¸½åƒæ•¸: {all_params:,}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {all_trainable:,} ({100*all_trainable/all_params:.4f}%)")
    print(f"  LoRA å±¤æ•¸: {total_lora}")
    print(f"  å¯è¨“ç·´ LoRA å±¤: {trainable_lora}")
    print(f"  Speech LoRA: r=320, alpha=640, dp=0.01")
    print(f"  Vision LoRA: r=256, alpha=512, dp=0.0")
    print(f"  ğŸ’¾ è¨“ç·´è¼¸å‡ºç›®éŒ„: output/pretrained_r320/")

    return model, processor, peft_config


def get_model_and_processor_paper(
    model_id: str = "microsoft/Phi-4-multimodal-instruct"
):
    """
    é…ç½® 2ï¼šè«–æ–‡ LoRA é…ç½®ï¼ˆr=64ï¼‰

    - Speech LoRA: r=64, alpha=128, dropout=0.05
    - Vision LoRA: r=256, alpha=512, dropout=0.0 (ä¿æŒé è¨“ç·´)
    - å¯è¨“ç·´åƒæ•¸: ~200M (3.5%)
    - ç‰¹é»: LoRA å¾é›¶è¨“ç·´ï¼Œåš´æ ¼å¾©ç¾è«–æ–‡è¨­å®š
    - è¼¸å‡ºç›®éŒ„: output/paper_r64/
    """

    bnb_config = None
    model_path = "/Users/xrickliao/WorkSpaces/LLM_Repo/models/Phi-4-multimodal-instruct"

    processor = AutoProcessor.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config = AutoConfig.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config._attn_implementation = "eager"

    # ä½¿ç”¨è«–æ–‡è¦æ ¼ï¼ˆSpeech LoRA r=64ï¼‰
    config.speech_lora = {
        'r': 64,              # è«–æ–‡è¦æ ¼
        'lora_alpha': 128,    # è«–æ–‡è¦æ ¼
        'layer': '((layers.*self_attn\\.(qkv|o)_proj)|(layers.*mlp\\.(gate_up|down)_proj))',
        'dp': 0.05            # è«–æ–‡è¦æ ¼
    }

    # Vision LoRA ä¿æŒé è¨“ç·´é…ç½®ï¼ˆéä¸»è¦ä»»å‹™ï¼‰
    config.vision_lora = {
        'r': 256,
        'lora_alpha': 512,
        'layer': 'layers.*((self_attn\\.(qkv_proj|o_proj))|(mlp\\.(gate_up|down)_proj))',
        'dp': 0.0
    }

    # ğŸ”‘ é—œéµï¼šignore_mismatched_sizes è®“ Speech LoRA é‡æ–°åˆå§‹åŒ–ç‚º r=64
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        config=config,
        local_files_only=True,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        ignore_mismatched_sizes=True,  # å…è¨±å½¢ç‹€ä¸åŒ¹é…ï¼ŒSpeech LoRA å¾é›¶åˆå§‹åŒ–
    )

    model.gradient_checkpointing_enable()

    peft_config = LoraConfig(
        r=64,
        lora_alpha=128,
        target_modules="all-linear",
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    # çµ±è¨ˆåƒæ•¸
    lora_params = [(name, p) for name, p in model.named_parameters() if "lora" in name.lower()]
    trainable_lora = sum(1 for _, p in lora_params if p.requires_grad)
    total_lora = len(lora_params)
    all_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())

    print(f"\nğŸ“Š ã€è«–æ–‡é…ç½® r=64ã€‘åƒæ•¸çµ±è¨ˆ:")
    print(f"  ç¸½åƒæ•¸: {all_params:,}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {all_trainable:,} ({100*all_trainable/all_params:.4f}%)")
    print(f"  LoRA å±¤æ•¸: {total_lora}")
    print(f"  å¯è¨“ç·´ LoRA å±¤: {trainable_lora}")
    print(f"  Speech LoRA: r=64, alpha=128, dp=0.05 (è«–æ–‡è¦æ ¼ï¼Œå¾é›¶è¨“ç·´)")
    print(f"  Vision LoRA: r=256, alpha=512, dp=0.0 (ä¿æŒé è¨“ç·´)")
    print(f"  ğŸ’¾ è¨“ç·´è¼¸å‡ºç›®éŒ„: output/paper_r64/")
    print(f"  âš ï¸  æ³¨æ„: Speech LoRA æ¬Šé‡è¢«é‡æ–°åˆå§‹åŒ–ï¼ˆå½¢ç‹€ä¸åŒ¹é…ï¼‰")

    return model, processor, peft_config


# é…ç½®å°ç…§è¡¨
CONFIGS = {
    "pretrained_r320": {
        "loader": get_model_and_processor_pretrained,
        "output_dir": "output/pretrained_r320",
        "description": "é è¨“ç·´ LoRA é…ç½®ï¼ˆr=320ï¼‰ï¼Œå¾é è¨“ç·´æ¬Šé‡é–‹å§‹",
        "speech_lora": {"r": 320, "alpha": 640, "dp": 0.01},
        "vision_lora": {"r": 256, "alpha": 512, "dp": 0.0},
        "trainable_params": "830M (14.9%)",
    },
    "paper_r64": {
        "loader": get_model_and_processor_paper,
        "output_dir": "output/paper_r64",
        "description": "è«–æ–‡ LoRA é…ç½®ï¼ˆr=64ï¼‰ï¼ŒSpeech LoRA å¾é›¶è¨“ç·´",
        "speech_lora": {"r": 64, "alpha": 128, "dp": 0.05},
        "vision_lora": {"r": 256, "alpha": 512, "dp": 0.0},
        "trainable_params": "~200M (3.5%)",
    }
}


def print_config_comparison():
    """æ‰“å°å…©ç¨®é…ç½®çš„å°ç…§è¡¨"""
    print("\n" + "="*80)
    print("LoRA é…ç½®å°ç…§è¡¨")
    print("="*80)
    print(f"{'é …ç›®':<20} {'é è¨“ç·´é…ç½® (r=320)':<35} {'è«–æ–‡é…ç½® (r=64)':<35}")
    print("-"*80)
    print(f"{'Speech LoRA rank':<20} {'320':<35} {'64 â­':<35}")
    print(f"{'Speech LoRA alpha':<20} {'640':<35} {'128 â­':<35}")
    print(f"{'Speech dropout':<20} {'0.01':<35} {'0.05 â­':<35}")
    print(f"{'Vision LoRA rank':<20} {'256':<35} {'256':<35}")
    print(f"{'Vision LoRA alpha':<20} {'512':<35} {'512':<35}")
    print(f"{'å¯è¨“ç·´åƒæ•¸':<20} {'830M (14.9%)':<35} {'~200M (3.5%)':<35}")
    print(f"{'è¨“ç·´èµ·é»':<20} {'é è¨“ç·´ LoRA æ¬Šé‡':<35} {'éš¨æ©Ÿåˆå§‹åŒ– â­':<35}")
    print(f"{'è¼¸å‡ºç›®éŒ„':<20} {'output/pretrained_r320/':<35} {'output/paper_r64/':<35}")
    print(f"{'é æœŸæ”¶æ–‚é€Ÿåº¦':<20} {'è¼ƒå¿«':<35} {'è¼ƒæ…¢ï¼ˆéœ€æ›´å¤š epochï¼‰':<35}")
    print(f"{'è«–æ–‡ç¬¦åˆåº¦':<20} {'éƒ¨åˆ†':<35} {'å®Œå…¨ç¬¦åˆ â­':<35}")
    print("="*80)
    print("â­ = è«–æ–‡åŸå§‹è¦æ ¼")
    print()
