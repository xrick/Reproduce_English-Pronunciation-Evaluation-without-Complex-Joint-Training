# ä½¿ç”¨è«–æ–‡åŸå§‹ LoRA è¦æ ¼å¾é›¶è¨“ç·´

## é…ç½®æ–¹æ¡ˆ

å¦‚æœè¦ä½¿ç”¨è«–æ–‡åŸå§‹è¦æ ¼ï¼ˆr=64, alpha=128, dropout=0.05ï¼‰ï¼Œéœ€è¦**ä¸è¼‰å…¥é è¨“ç·´ LoRA æ¬Šé‡**ï¼Œæ”¹ç‚ºå¾é›¶é–‹å§‹è¨“ç·´ã€‚

## å¯¦ä½œæ–¹æ³•

### é¸é … Aï¼šä¿®æ”¹æ¨¡å‹è¼‰å…¥é‚è¼¯ï¼ˆè·³é LoRA æ¬Šé‡ï¼‰

```python
# åœ¨ model_utility.py ä¸­ä¿®æ”¹

# Speech LoRA: ä½¿ç”¨è«–æ–‡è¦æ ¼
config.speech_lora = {
    'r': 64,              # è«–æ–‡è¦æ ¼
    'lora_alpha': 128,    # è«–æ–‡è¦æ ¼
    'layer': '((layers.*self_attn\\.(qkv|o)_proj)|(layers.*mlp\\.(gate_up|down)_proj))',
    'dp': 0.05            # è«–æ–‡è¦æ ¼
}

config.vision_lora = {
    'r': 64,              # è«–æ–‡è¦æ ¼ï¼ˆæˆ–ä¿æŒ 256ï¼‰
    'lora_alpha': 128,    # è«–æ–‡è¦æ ¼ï¼ˆæˆ–ä¿æŒ 512ï¼‰
    'layer': 'layers.*((self_attn\\.(qkv_proj|o_proj))|(mlp\\.(gate_up|down)_proj))',
    'dp': 0.05            # è«–æ–‡è¦æ ¼
}

# è¼‰å…¥æ¨¡å‹æ™‚è·³é LoRA æ¬Šé‡
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,
    local_files_only=True,
    quantization_config=None,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    ignore_mismatched_sizes=True,  # ğŸ”‘ é—œéµï¼šå¿½ç•¥å½¢ç‹€ä¸åŒ¹é…
)
```

### é¸é … Bï¼šä¸è¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼ˆå®Œå…¨å¾é›¶ï¼‰

```python
from transformers import AutoConfig, AutoModelForCausalLM

# åªè¼‰å…¥é…ç½®ï¼Œä¸è¼‰å…¥æ¬Šé‡
config = AutoConfig.from_pretrained(
    model_path,
    local_files_only=True,
    trust_remote_code=True
)

# è¨­å®š LoRA ç‚ºè«–æ–‡è¦æ ¼
config.speech_lora = {'r': 64, 'lora_alpha': 128, 'dp': 0.05, ...}
config.vision_lora = {'r': 64, 'lora_alpha': 128, 'dp': 0.05, ...}

# å¾é…ç½®å»ºç«‹æ–°æ¨¡å‹ï¼ˆéš¨æ©Ÿåˆå§‹åŒ–ï¼‰
model = AutoModelForCausalLM.from_config(
    config,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)
```

### é¸é … Cï¼šè¼‰å…¥åŸºç¤æ¨¡å‹ï¼Œé‡æ–°åˆå§‹åŒ– LoRA

```python
# 1. å…ˆè¼‰å…¥ä¸å« LoRA çš„åŸºç¤æ¨¡å‹
config_no_lora = AutoConfig.from_pretrained(model_path, ...)
# æš«æ™‚ç¦ç”¨ LoRAï¼ˆè¨­ç‚ºæœ€å°å€¼ï¼‰
config_no_lora.speech_lora = {'r': 1, ...}  # æœ€å°åŒ– LoRA
config_no_lora.vision_lora = {'r': 1, ...}

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config_no_lora,
    ...
)

# 2. æ‰‹å‹•é‡æ–°åˆå§‹åŒ–ç‚ºè«–æ–‡è¦æ ¼çš„ LoRA
# ï¼ˆéœ€è¦ä¿®æ”¹æ¨¡å‹å…§éƒ¨çµæ§‹ï¼Œè¼ƒè¤‡é›œï¼‰
```

## å„ªç¼ºé»æ¯”è¼ƒ

| æ–¹æ¡ˆ | å„ªé» | ç¼ºé» | VRAM | è¨“ç·´æ™‚é–“ |
|------|------|------|------|---------|
| **A: ignore_mismatched_sizes** | ç°¡å–®ï¼Œä¸€è¡Œä»£ç¢¼ | LoRA éš¨æ©Ÿåˆå§‹åŒ–ï¼Œå¤±å»é è¨“ç·´å„ªå‹¢ | ~40GB | é•·ï¼ˆå¾é›¶è¨“ç·´ï¼‰|
| **B: from_config** | å®Œå…¨æ§åˆ¶ï¼Œæ¸…æ™°æ˜ç¢º | æ•´å€‹æ¨¡å‹å¾é›¶è¨“ç·´ï¼ˆåŒ…æ‹¬åŸºç¤å±¤ï¼‰ | ~40GB | æœ€é•· |
| **C: é‡æ–°åˆå§‹åŒ–** | ä¿ç•™åŸºç¤æ¨¡å‹æ¬Šé‡ | å¯¦ä½œè¤‡é›œï¼Œéœ€ä¿®æ”¹å…§éƒ¨çµæ§‹ | ~40GB | é•·ï¼ˆLoRA å¾é›¶ï¼‰|

## æ¨è–¦æ–¹æ¡ˆï¼šé¸é … A

### ç‚ºä»€éº¼æ¨è–¦é¸é … Aï¼Ÿ

1. **å¯¦ä½œç°¡å–®**ï¼šåªéœ€åŠ ä¸€å€‹åƒæ•¸ `ignore_mismatched_sizes=True`
2. **ä¿ç•™åŸºç¤æ¨¡å‹**ï¼šPhi-4 çš„ä¸»é«”æ¬Šé‡ï¼ˆLLMã€è¦–è¦ºã€éŸ³è¨Šç·¨ç¢¼å™¨ï¼‰ä»ä½¿ç”¨é è¨“ç·´
3. **LoRA å¾é›¶è¨“ç·´**ï¼šç¬¦åˆè«–æ–‡è¨­å®šï¼Œå…¬å¹³æ¯”è¼ƒ

### å®Œæ•´å¯¦ä½œä»£ç¢¼

```python
def get_model_and_processor(
    model_id: str = "microsoft/Phi-4-multimodal-instruct",
    lora_rank: int = 64,        # è«–æ–‡è¦æ ¼
    lora_alpha: int = 128       # è«–æ–‡è¦æ ¼
):
    bnb_config = None  # bfloat16ï¼Œç„¡é‡åŒ–

    model_path = "/Users/xrickliao/WorkSpaces/LLM_Repo/models/Phi-4-multimodal-instruct"

    processor = AutoProcessor.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config = AutoConfig.from_pretrained(
        model_path,
        local_files_only=True,
        trust_remote_code=True
    )

    config._attn_implementation = "eager"

    # ğŸ”‘ ä½¿ç”¨è«–æ–‡è¦æ ¼
    config.speech_lora = {
        'r': lora_rank,           # 64
        'lora_alpha': lora_alpha, # 128
        'layer': '((layers.*self_attn\\.(qkv|o)_proj)|(layers.*mlp\\.(gate_up|down)_proj))',
        'dp': 0.05                # è«–æ–‡è¦æ ¼
    }

    config.vision_lora = {
        'r': lora_rank,           # 64ï¼ˆæˆ– 256 ä¿æŒåŸæ¨£ï¼‰
        'lora_alpha': lora_alpha, # 128ï¼ˆæˆ– 512 ä¿æŒåŸæ¨£ï¼‰
        'layer': 'layers.*((self_attn\\.(qkv_proj|o_proj))|(mlp\\.(gate_up|down)_proj))',
        'dp': 0.05
    }

    # ğŸ”‘ é—œéµï¼šå¿½ç•¥å½¢ç‹€ä¸åŒ¹é…ï¼ŒLoRA æ¬Šé‡æœƒè¢«é‡æ–°åˆå§‹åŒ–
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        config=config,
        local_files_only=True,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        ignore_mismatched_sizes=True,  # ğŸ”‘ å…è¨±å½¢ç‹€ä¸åŒ¹é…
    )

    model.gradient_checkpointing_enable()

    # LoRA åƒæ•¸æœƒè¢«éš¨æ©Ÿåˆå§‹åŒ–ç‚ºè«–æ–‡è¦æ ¼
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules="all-linear",
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    return model, processor, peft_config
```

## é æœŸçµæœ

åŸ·è¡Œå¾Œæ‡‰è©²çœ‹åˆ°ï¼š

```
âš ï¸  Some weights were not initialized from checkpoint: ...lora_A...lora_B...
âœ… å¯è¨“ç·´ LoRA å±¤: 512 å€‹
âœ… LoRA rank: 64 (è«–æ–‡è¦æ ¼)
âœ… å¯è¨“ç·´åƒæ•¸: ~200M (ç´„ 3.5%)  # æ¯” r=320 å°‘å¾ˆå¤š
```

## è¨“ç·´è€ƒé‡

### å„ªé»
- âœ… å®Œå…¨ç¬¦åˆè«–æ–‡å¯¦é©—è¨­å®š
- âœ… å¯å…¬å¹³æ¯”è¼ƒè«–æ–‡çµæœ
- âœ… åƒæ•¸æ›´å°‘ï¼Œè¨“ç·´æ›´å¿«

### ç¼ºé»
- âš ï¸ å¤±å»é è¨“ç·´ LoRA çš„å„ªå‹¢
- âš ï¸ å¯èƒ½éœ€è¦æ›´å¤šè¨“ç·´ epoch æ‰èƒ½æ”¶æ–‚
- âš ï¸ åˆæœŸæ€§èƒ½æœƒè¼ƒå·®ï¼ˆéœ€è¦å¾é›¶å­¸ç¿’ï¼‰

### å»ºè­°
1. **å…ˆè©¦è©¦é è¨“ç·´é…ç½®**ï¼ˆr=320ï¼‰è¨“ç·´å¹¾å€‹ epochï¼Œçœ‹çœ‹æ•ˆæœ
2. **å¦‚æœè¦åš´æ ¼å¾©ç¾è«–æ–‡**ï¼Œä½¿ç”¨ r=64 å¾é›¶è¨“ç·´
3. **è¨˜éŒ„å…©ç¨®é…ç½®çš„æ€§èƒ½**ï¼Œæ¯”è¼ƒå·®ç•°

## Vision LoRA é…ç½®å»ºè­°

è«–æ–‡ä¸»è¦é—œæ³¨ **Speech LoRA**ï¼ˆèªéŸ³ç™¼éŸ³è©•ä¼°ä»»å‹™ï¼‰ï¼ŒVision LoRA å¯èƒ½å½±éŸ¿ä¸å¤§ã€‚å»ºè­°ï¼š

**é¸é … 1**ï¼ˆä¿å®ˆï¼‰ï¼š
```python
config.vision_lora = {
    'r': 256,      # ä¿æŒé è¨“ç·´å€¼
    'lora_alpha': 512,
    'dp': 0.0
}
```

**é¸é … 2**ï¼ˆåš´æ ¼å¾©ç¾ï¼‰ï¼š
```python
config.vision_lora = {
    'r': 64,       # è«–æ–‡è¦æ ¼
    'lora_alpha': 128,
    'dp': 0.05
}
```

æ¨è–¦**é¸é … 1**ï¼Œå› ç‚ºè¦–è¦ºæ¨¡æ…‹ä¸æ˜¯ä¸»è¦ä»»å‹™ï¼Œä¿ç•™é è¨“ç·´å„ªå‹¢è¼ƒå®‰å…¨ã€‚
