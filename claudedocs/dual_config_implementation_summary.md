# é›™é…ç½®è¨“ç·´ç³»çµ±å¯¦ä½œç¸½çµ

**æ—¥æœŸ**: 2025-12-20
**ç‹€æ…‹**: âœ… å®Œæˆå¯¦ä½œï¼Œæº–å‚™è¨“ç·´

---

## å¯¦ä½œæ¦‚è¿°

æˆåŠŸå¯¦ä½œäº†é›™ LoRA é…ç½®è¨“ç·´ç³»çµ±ï¼Œæ”¯æ´ï¼š

1. **é è¨“ç·´é…ç½®** (r=320)ï¼šä½¿ç”¨ Phi-4-multimodal çš„é è¨“ç·´ LoRA æ¬Šé‡
2. **è«–æ–‡é…ç½®** (r=64)ï¼šå¾é›¶è¨“ç·´ï¼Œåš´æ ¼å¾©ç¾è«–æ–‡è¦æ ¼

å…©ç¨®é…ç½®å¯ä»¥ç¨ç«‹è¨“ç·´ï¼Œæ¨¡å‹ä¿å­˜åˆ°ä¸åŒçš„è¼¸å‡ºç›®éŒ„ï¼Œä¾¿æ–¼æ€§èƒ½æ¯”è¼ƒå’Œåˆ†æã€‚

---

## æ ¸å¿ƒæª”æ¡ˆ

### 1. æ¨¡å‹é…ç½®è¼‰å…¥å™¨

**æª”æ¡ˆ**: [src/model_utility_configs.py](../src/model_utility_configs.py)

**åŠŸèƒ½**:
- `get_model_and_processor_pretrained()`: è¼‰å…¥é è¨“ç·´é…ç½® (r=320)
- `get_model_and_processor_paper()`: è¼‰å…¥è«–æ–‡é…ç½® (r=64)
- `CONFIGS`: é…ç½®å­—å…¸ï¼Œæ˜ å°„é…ç½®åç¨±åˆ°è¼‰å…¥å‡½æ•¸å’Œå…ƒæ•¸æ“š
- `print_config_comparison()`: æ‰“å°é…ç½®å°ç…§è¡¨

**é—œéµå¯¦ä½œç´°ç¯€**:

```python
# é è¨“ç·´é…ç½®ï¼šç›´æ¥è¼‰å…¥é è¨“ç·´ LoRA æ¬Šé‡
config.speech_lora = {'r': 320, 'lora_alpha': 640, 'dp': 0.01, ...}
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,
    torch_dtype=torch.bfloat16,
    # ä¸ä½¿ç”¨ ignore_mismatched_sizes
)

# è«–æ–‡é…ç½®ï¼šå…è¨±å½¢ç‹€ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ– Speech LoRA
config.speech_lora = {'r': 64, 'lora_alpha': 128, 'dp': 0.05, ...}
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,
    torch_dtype=torch.bfloat16,
    ignore_mismatched_sizes=True,  # ğŸ”‘ é—œéµåƒæ•¸
)
```

**PEFT è£œä¸** (å…©ç¨®é…ç½®å…±ç”¨):
```python
def _patched_peft_init(self, model, peft_config, adapter_name="default", **kwargs):
    if not hasattr(model, 'prepare_inputs_for_generation'):
        def prepare_inputs_for_generation(*args, **kwargs):
            return {}
        model.prepare_inputs_for_generation = prepare_inputs_for_generation
    _original_peft_init(self, model, peft_config, adapter_name, **kwargs)
```

### 2. å–®ä¸€é…ç½®è¨“ç·´è…³æœ¬

**æª”æ¡ˆ**: [src/train_single_config.py](../src/train_single_config.py)

**åŠŸèƒ½**:
- å‘½ä»¤è¡Œä»‹é¢è¨“ç·´å–®ä¸€é…ç½®
- æ”¯æ´è‡ªå®šç¾©è¨“ç·´è¶…åƒæ•¸
- è‡ªå‹•ä¿å­˜è¨“ç·´é…ç½® JSON
- TensorBoard æ—¥èªŒè¨˜éŒ„

**ä½¿ç”¨æ–¹å¼**:
```bash
python train_single_config.py --config pretrained_r320
python train_single_config.py --config paper_r64

# è‡ªå®šç¾©åƒæ•¸
python train_single_config.py \
    --config paper_r64 \
    --epochs 4 \
    --batch-size 4 \
    --gradient-accumulation 16
```

### 3. é›™é…ç½®äº¤äº’å¼è¨“ç·´è…³æœ¬

**æª”æ¡ˆ**: [src/train_dual_configs.py](../src/train_dual_configs.py)

**åŠŸèƒ½**:
- äº¤äº’å¼é¸æ“‡è¨“ç·´é †åº
- æ”¯æ´è¨“ç·´å–®ä¸€æˆ–å…©ç¨®é…ç½®
- é¡¯ç¤ºé…ç½®å°ç…§è¡¨
- è‡ªå‹•ä¿å­˜è¨“ç·´é…ç½®å’Œå…ƒæ•¸æ“š

**è¨“ç·´é¸é …**:
1. å…ˆè¨“ç·´é è¨“ç·´é…ç½®ï¼Œå†è¨“ç·´è«–æ–‡é…ç½®
2. å…ˆè¨“ç·´è«–æ–‡é…ç½®ï¼Œå†è¨“ç·´é è¨“ç·´é…ç½®
3. åƒ…è¨“ç·´é è¨“ç·´é…ç½®
4. åƒ…è¨“ç·´è«–æ–‡é…ç½®

### 4. å¿«é€Ÿå•Ÿå‹• Shell è…³æœ¬

**æª”æ¡ˆ**: [train_both_configs.sh](../train_both_configs.sh)

**åŠŸèƒ½**:
- ä¸€éµå•Ÿå‹•è¨“ç·´
- æ”¯æ´è¨“ç·´å–®ä¸€æˆ–å…©ç¨®é…ç½®
- è‡ªå‹•æ¿€æ´»è™›æ“¬ç’°å¢ƒ

**ä½¿ç”¨æ–¹å¼**:
```bash
./train_both_configs.sh              # è¨“ç·´å…©ç¨®é…ç½®
./train_both_configs.sh pretrained   # åƒ…è¨“ç·´é è¨“ç·´é…ç½®
./train_both_configs.sh paper        # åƒ…è¨“ç·´è«–æ–‡é…ç½®
```

### 5. é…ç½®é©—è­‰è…³æœ¬

**æª”æ¡ˆ**: [src/verify_configs.py](../src/verify_configs.py)

**åŠŸèƒ½**:
- é©—è­‰å…©ç¨®é…ç½®æ˜¯å¦æ­£ç¢ºè¼‰å…¥
- æª¢æŸ¥ LoRA åƒæ•¸å¯è¨“ç·´æ€§
- é¡¯ç¤ºåƒæ•¸çµ±è¨ˆ
- é©—è­‰é…ç½®è¨­ç½®

**ä½¿ç”¨æ–¹å¼**:
```bash
source run_env.sh
cd src
python verify_configs.py
```

---

## é…ç½®è¦æ ¼

### é è¨“ç·´é…ç½® (pretrained_r320)

| åƒæ•¸ | å€¼ |
|------|-----|
| Speech LoRA rank | 320 |
| Speech LoRA alpha | 640 |
| Speech LoRA dropout | 0.01 |
| Vision LoRA rank | 256 |
| Vision LoRA alpha | 512 |
| Vision LoRA dropout | 0.0 |
| å¯è¨“ç·´åƒæ•¸ | 830M (14.9%) |
| è¨“ç·´èµ·é» | é è¨“ç·´ LoRA æ¬Šé‡ |
| è¼¸å‡ºç›®éŒ„ | output/pretrained_r320/ |

### è«–æ–‡é…ç½® (paper_r64)

| åƒæ•¸ | å€¼ |
|------|-----|
| Speech LoRA rank | 64 â­ |
| Speech LoRA alpha | 128 â­ |
| Speech LoRA dropout | 0.05 â­ |
| Vision LoRA rank | 256 |
| Vision LoRA alpha | 512 |
| Vision LoRA dropout | 0.0 |
| å¯è¨“ç·´åƒæ•¸ | ~200M (3.5%) |
| è¨“ç·´èµ·é» | éš¨æ©Ÿåˆå§‹åŒ– â­ |
| è¼¸å‡ºç›®éŒ„ | output/paper_r64/ |

â­ = è«–æ–‡åŸå§‹è¦æ ¼

---

## è¨“ç·´è¶…åƒæ•¸ï¼ˆè«–æ–‡è¦æ ¼ï¼‰

åŸºæ–¼è«–æ–‡ Table 3 çš„æœ€ä½³é…ç½®ï¼š

```python
num_train_epochs = 3                    # è«–æ–‡æœ€ä½³çµæœåœ¨ epoch 3
per_device_train_batch_size = 8         # è«–æ–‡è¨­å®š
gradient_accumulation_steps = 8         # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 64
learning_rate = 2e-5                    # è«–æ–‡è¨­å®š (2Ã—10â»âµ)
optimizer = "adamw_torch"               # Adam å„ªåŒ–å™¨
bf16 = True                             # bfloat16 ç²¾åº¦
max_length = 2048                       # éŸ³è¨Š token å®¹é‡ (SFTConfig ä½¿ç”¨ max_length)
gradient_checkpointing = True           # å…§å­˜å„ªåŒ–
```

---

## è¼¸å‡ºçµæ§‹

è¨“ç·´å¾Œçš„ç›®éŒ„çµæ§‹ï¼š

```
output/
â”œâ”€â”€ pretrained_r320/
â”‚   â”œâ”€â”€ checkpoint-epoch-1/
â”‚   â”œâ”€â”€ checkpoint-epoch-2/
â”‚   â”œâ”€â”€ checkpoint-epoch-3/
â”‚   â”œâ”€â”€ final_model/
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ processor_config.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ events.out.tfevents.*
â”‚   â””â”€â”€ training_config.json
â”‚
â””â”€â”€ paper_r64/
    â”œâ”€â”€ checkpoint-epoch-1/
    â”œâ”€â”€ checkpoint-epoch-2/
    â”œâ”€â”€ checkpoint-epoch-3/
    â”œâ”€â”€ final_model/
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â”œâ”€â”€ adapter_model.safetensors
    â”‚   â”œâ”€â”€ processor_config.json
    â”‚   â”œâ”€â”€ tokenizer_config.json
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ events.out.tfevents.*
    â””â”€â”€ training_config.json
```

---

## æŠ€è¡“ç´°ç¯€

### é—œéµå·®ç•°ï¼šignore_mismatched_sizes

**é è¨“ç·´é…ç½®**:
- æ¨¡å‹è¼‰å…¥æ™‚ä¸ä½¿ç”¨ `ignore_mismatched_sizes`
- Speech LoRA æ¬Šé‡å¾ checkpoint è¼‰å…¥ (r=320)
- Vision LoRA æ¬Šé‡å¾ checkpoint è¼‰å…¥ (r=256)
- **çµæœ**: ä½¿ç”¨é è¨“ç·´çš„ LoRA æ¬Šé‡

**è«–æ–‡é…ç½®**:
- æ¨¡å‹è¼‰å…¥æ™‚ä½¿ç”¨ `ignore_mismatched_sizes=True`
- Speech LoRA æ¬Šé‡å› å½¢ç‹€ä¸åŒ¹é…è¢«é‡æ–°åˆå§‹åŒ– (r=64)
- Vision LoRA æ¬Šé‡å¾ checkpoint è¼‰å…¥ (r=256ï¼Œå½¢ç‹€ç›¸åŒ)
- **çµæœ**: Speech LoRA å¾é›¶è¨“ç·´ï¼ŒVision LoRA ä½¿ç”¨é è¨“ç·´æ¬Šé‡

### PEFT è£œä¸

å…©ç¨®é…ç½®éƒ½ä½¿ç”¨ç›¸åŒçš„ PEFT è£œä¸ä¾†è§£æ±º Phi-4-multimodal çš„æ¶æ§‹ä¸å…¼å®¹å•é¡Œï¼š

**å•é¡Œ**: `Phi4MMModel` ç¼ºå°‘ `prepare_inputs_for_generation` æ–¹æ³•
**è§£æ±ºæ–¹æ¡ˆ**: åœ¨ PEFT åˆå§‹åŒ–å‰å‹•æ…‹æ·»åŠ è©²æ–¹æ³•
**å¯¦ä½œä½ç½®**: [src/model_utility_configs.py:14-30](../src/model_utility_configs.py)

### ç²¾åº¦é¸æ“‡ï¼šbfloat16 ç„¡é‡åŒ–

**ç‚ºä»€éº¼ä¸ä½¿ç”¨é‡åŒ–ï¼Ÿ**
- 4-bit/8-bit é‡åŒ–æœƒå°‡ LoRA åƒæ•¸ä¹Ÿé‡åŒ–ç‚º uint8/int8
- é‡åŒ–å¾Œçš„ LoRA åƒæ•¸ç„¡æ³•è¨­ç½® `requires_grad=True`
- å°è‡´ LoRA å±¤ç„¡æ³•è¨“ç·´

**è§£æ±ºæ–¹æ¡ˆ**:
- ä½¿ç”¨ `torch_dtype=torch.bfloat16`
- ä¸ä½¿ç”¨ `quantization_config`
- LoRA åƒæ•¸ä¿æŒç‚º bfloat16ï¼Œå¯ä»¥æ­£å¸¸è¨“ç·´

**VRAM å½±éŸ¿**:
- ç„¡é‡åŒ– bfloat16: ~40-45GB
- å»ºè­°ç¡¬é«”: NVIDIA A100 (80GB) æˆ– A6000 (48GB)

---

## æ–‡æª”ç³»çµ±

### ç”¨æˆ¶æ–‡æª”

1. **[claudedocs/dual_config_training_guide.md](dual_config_training_guide.md)**
   - å®Œæ•´çš„è¨“ç·´æŒ‡å—
   - é…ç½®å°ç…§è¡¨
   - è¨“ç·´åƒæ•¸èªªæ˜
   - å¸¸è¦‹å•é¡Œè§£ç­”

2. **[claudedocs/training_quick_reference.md](training_quick_reference.md)**
   - å¿«é€Ÿåƒè€ƒå¡
   - ä¸€é ç¸½çµæ‰€æœ‰é—œéµä¿¡æ¯

3. **[CLAUDE.md](../CLAUDE.md)** (å·²æ›´æ–°)
   - å°ˆæ¡ˆç¸½è¦½
   - æ–°å¢é›™é…ç½®ç³»çµ±èªªæ˜
   - å¿«é€Ÿå•Ÿå‹•æŒ‡å—

### æŠ€è¡“æ–‡æª”

1. **[claudedocs/peft_lora_incompatibility.md](peft_lora_incompatibility.md)**
   - PEFT/LoRA ä¸å…¼å®¹å•é¡Œè©³ç´°æ–‡æª”
   - å˜—è©¦çš„è§£æ±ºæ–¹æ¡ˆæ­·å²
   - æœ€çµ‚è§£æ±ºæ–¹æ¡ˆèªªæ˜

2. **[claudedocs/lora_from_scratch_config.md](lora_from_scratch_config.md)**
   - è«–æ–‡è¦æ ¼ (r=64) å¾é›¶è¨“ç·´æŒ‡å—
   - ä¸‰ç¨®å¯¦ä½œé¸é …æ¯”è¼ƒ
   - æ¨è–¦æ–¹æ¡ˆå’Œå¯¦ä½œä»£ç¢¼

3. **æœ¬æ–‡æª”** (dual_config_implementation_summary.md)
   - å¯¦ä½œç¸½çµ
   - æŠ€è¡“ç´°ç¯€è¨˜éŒ„
   - è¨­è¨ˆæ±ºç­–èªªæ˜

---

## é©—è­‰æ¸…å–®

åœ¨é–‹å§‹è¨“ç·´å‰ï¼Œè«‹ç¢ºèªï¼š

- [ ] è™›æ“¬ç’°å¢ƒå·²æ¿€æ´» (`source run_env.sh`)
- [ ] è¨“ç·´æ•¸æ“šé›†å·²æº–å‚™ (`../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/train/`)
- [ ] æ¸¬è©¦æ•¸æ“šé›†å·²æº–å‚™ (`../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/test/`)
- [ ] æ¨¡å‹æ¬Šé‡å·²ä¸‹è¼‰ (`/Users/xrickliao/WorkSpaces/LLM_Repo/models/Phi-4-multimodal-instruct/`)
- [ ] é…ç½®é©—è­‰é€šé (`python src/verify_configs.py`)
- [ ] è¶³å¤ çš„ VRAM (â‰¥40GB)
- [ ] è¶³å¤ çš„ç£ç¢Ÿç©ºé–“ (æ¯å€‹é…ç½®ç´„ 5-10GB)

---

## é æœŸè¨“ç·´æ™‚é–“

åŸºæ–¼ 3 epochs è¨“ç·´ï¼š

- **é è¨“ç·´é…ç½®** (r=320): ç´„ 18-24 å°æ™‚
  - æ›´å¤šå¯è¨“ç·´åƒæ•¸ (830M)
  - ä½†å¾é è¨“ç·´æ¬Šé‡é–‹å§‹ï¼Œæ”¶æ–‚è¼ƒå¿«

- **è«–æ–‡é…ç½®** (r=64): ç´„ 12-18 å°æ™‚
  - è¼ƒå°‘å¯è¨“ç·´åƒæ•¸ (~200M)
  - ä½†å¾é›¶è¨“ç·´ï¼Œå¯èƒ½éœ€è¦æ›´å¤š epoch æ‰èƒ½é”åˆ°æœ€ä½³æ€§èƒ½

**ç¸½è¨ˆ**ï¼ˆè¨“ç·´å…©ç¨®é…ç½®ï¼‰: ç´„ 30-42 å°æ™‚

---

## ä¸‹ä¸€æ­¥

### 1. é©—è­‰é…ç½®

```bash
source run_env.sh
cd src
python verify_configs.py
```

### 2. é–‹å§‹è¨“ç·´

```bash
# æ–¹å¼ 1: ä¸€éµè¨“ç·´å…©ç¨®é…ç½®
./train_both_configs.sh

# æ–¹å¼ 2: åˆ†åˆ¥è¨“ç·´
./train_both_configs.sh paper      # å…ˆè¨“ç·´è«–æ–‡é…ç½®
./train_both_configs.sh pretrained # å†è¨“ç·´é è¨“ç·´é…ç½®
```

### 3. ç›£æ§è¨“ç·´

```bash
# æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
tensorboard --logdir output/

# æŸ¥çœ‹ç‰¹å®šé…ç½®
tensorboard --logdir output/paper_r64/logs
```

### 4. è©•ä¼°æ¨¡å‹

```bash
# è©•ä¼°è«–æ–‡é…ç½®
python estimate.py \
    --model-path ../output/paper_r64/final_model \
    --test-data ../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/test/

# è©•ä¼°é è¨“ç·´é…ç½®
python estimate.py \
    --model-path ../output/pretrained_r320/final_model \
    --test-data ../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/test/
```

### 5. æ¯”è¼ƒæ€§èƒ½

æ¯”è¼ƒå…©ç¨®é…ç½®çš„ï¼š
- PCC (Accuracy, Fluency, Prosodic, Total)
- WER, PER, F1-score
- è¨“ç·´æ™‚é–“å’Œæ”¶æ–‚é€Ÿåº¦
- èˆ‡è«–æ–‡åŸºæº–çš„å·®è·

---

## è«–æ–‡åŸºæº–æ€§èƒ½ (Paper Table 3)

LoRA-only é…ç½®ï¼ŒEpoch 3ï¼š

| æŒ‡æ¨™ | ç›®æ¨™å€¼ |
|------|--------|
| Accuracy PCC | 0.656 |
| Fluency PCC | 0.727 |
| Prosodic PCC | 0.711 |
| Total PCC | 0.675 |
| WER | 0.140 |
| PER | 0.114 |
| F1-score | 0.724 |

---

## è¨­è¨ˆæ±ºç­–

### ç‚ºä»€éº¼éœ€è¦å…©ç¨®é…ç½®ï¼Ÿ

1. **ç§‘å­¸é©—è­‰**: è«–æ–‡é…ç½®ç”¨æ–¼åš´æ ¼å¾©ç¾è«–æ–‡çµæœ
2. **æ€§èƒ½æ¢ç´¢**: é è¨“ç·´é…ç½®æ¢ç´¢é è¨“ç·´ LoRA çš„æ€§èƒ½ä¸Šé™
3. **æ¯”è¼ƒåˆ†æ**: è©•ä¼°é è¨“ç·´ LoRA çš„å¯¦éš›åƒ¹å€¼
4. **éˆæ´»æ€§**: ç”¨æˆ¶å¯ä»¥æ ¹æ“šéœ€æ±‚é¸æ“‡åˆé©çš„é…ç½®

### ç‚ºä»€éº¼ä¿ç•™ Vision LoRA r=256ï¼Ÿ

è«–æ–‡ä¸»è¦é—œæ³¨ Speech LoRAï¼ˆèªéŸ³ç™¼éŸ³è©•ä¼°ä»»å‹™ï¼‰ï¼ŒVision LoRA å½±éŸ¿è¼ƒå°ï¼š

- **ä¿å®ˆç­–ç•¥**: ä¿ç•™ Vision LoRA é è¨“ç·´å€¼ (r=256)ï¼Œé™ä½é¢¨éšª
- **ä¸»è¦ä»»å‹™**: èªéŸ³è©•ä¼°ç‚ºä¸»ï¼Œè¦–è¦ºæ¨¡æ…‹ç‚ºè¼”
- **ç©©å®šæ€§**: é¿å…åŒæ™‚å¾é›¶è¨“ç·´å…©å€‹ LoRAï¼Œå¢åŠ è¨“ç·´ç©©å®šæ€§

### ç‚ºä»€éº¼ä½¿ç”¨ bfloat16 è€Œéé‡åŒ–ï¼Ÿ

- **LoRA å¯è¨“ç·´æ€§**: é‡åŒ–æœƒå°è‡´ LoRA åƒæ•¸ç„¡æ³•è¨“ç·´
- **ç²¾åº¦è¦æ±‚**: ç™¼éŸ³è©•ä¼°éœ€è¦è¼ƒé«˜ç²¾åº¦
- **ç¡¬é«”å¯ç”¨æ€§**: A100/A6000 æä¾›è¶³å¤ çš„ VRAM

---

## å·²çŸ¥é™åˆ¶

1. **VRAM éœ€æ±‚é«˜**: éœ€è¦ 40-45GB VRAMï¼Œé™åˆ¶å¯ç”¨ç¡¬é«”
2. **è¨“ç·´æ™‚é–“é•·**: å–®å€‹é…ç½®éœ€è¦ 12-24 å°æ™‚
3. **æ•¸æ“šé›†é™åˆ¶**: åƒ…åœ¨ SpeechOcean762 ä¸Šæ¸¬è©¦
4. **ç¼ºå°‘æ§åˆ¶ token**: ç•¶å‰å¯¦ä½œå°šæœªåŠ å…¥ `<|APA|>` å’Œ `<|MDD|>` æ§åˆ¶ token
5. **æç¤ºå·¥ç¨‹ä¸å®Œæ•´**: å°šæœªåŠ å…¥è«–æ–‡çš„è©³ç´°è©•åˆ†æ¨™æº– (133 è¡Œ)
6. **æ¨™ç±¤é®ç½©æœªå¯¦ä½œ**: éœ€è¦å¯¦ä½œ prompt maskingï¼ˆåƒ…è¨“ç·´ assistant å›æ‡‰éƒ¨åˆ†ï¼‰

---

## æœªä¾†æ”¹é€²

### çŸ­æœŸï¼ˆè¨“ç·´å‰å¿…é ˆå®Œæˆï¼‰

1. **åŠ å…¥æ§åˆ¶ token**: å¯¦ä½œ `<|APA|>` å’Œ `<|MDD|>` token
2. **è©³ç´° prompt**: åŠ å…¥è«–æ–‡é™„éŒ„ 7.1 çš„å®Œæ•´è©•åˆ†æ¨™æº–
3. **Prompt masking**: å¯¦ä½œæ¨™ç±¤é®ç½©ï¼Œåªè¨“ç·´ assistant å›æ‡‰

### ä¸­æœŸï¼ˆæå‡æ€§èƒ½ï¼‰

4. **å®Œæ•´è©•ä¼°æŒ‡æ¨™**: å¯¦ä½œæ‰€æœ‰ PCCã€WERã€PERã€F1 æŒ‡æ¨™
5. **Checkpoint æ¢å¾©**: æ”¯æ´å¾ checkpoint ç¹¼çºŒè¨“ç·´
6. **å­¸ç¿’ç‡èª¿åº¦**: å¯¦ä½œ warmup å’Œ decay

### é•·æœŸï¼ˆç ”ç©¶æ“´å±•ï¼‰

7. **å¤šæ•¸æ“šé›†é©—è­‰**: åœ¨å…¶ä»–ç™¼éŸ³è©•ä¼°æ•¸æ“šé›†ä¸Šæ¸¬è©¦
8. **æ··åˆç²¾åº¦å„ªåŒ–**: æ¢ç´¢ FP8 æˆ–å…¶ä»–æ··åˆç²¾åº¦æ–¹æ¡ˆ
9. **åˆ†ä½ˆå¼è¨“ç·´**: æ”¯æ´å¤š GPU è¨“ç·´ï¼Œç¸®çŸ­è¨“ç·´æ™‚é–“

---

## è²¢ç»è€…

- **å¯¦ä½œè€…**: Claude (Anthropic)
- **æŒ‡å°**: xrickliao
- **æ—¥æœŸ**: 2025-12-20

---

## åƒè€ƒè³‡æ–™

- [è«–æ–‡åŸæ–‡](../paper/)
- [å°ˆæ¡ˆ CLAUDE.md](../CLAUDE.md)
- [PEFT ä¸å…¼å®¹å•é¡Œæ–‡æª”](peft_lora_incompatibility.md)
- [å¾é›¶è¨“ç·´é…ç½®æŒ‡å—](lora_from_scratch_config.md)
- [å®Œæ•´è¨“ç·´æŒ‡å—](dual_config_training_guide.md)
