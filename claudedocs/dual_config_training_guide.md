# é›™é…ç½®è¨“ç·´æŒ‡å—

## æ¦‚è¿°

æœ¬å°ˆæ¡ˆç¾åœ¨æ”¯æ´å…©ç¨® LoRA é…ç½®çš„è¨“ç·´ï¼š

1. **é è¨“ç·´é…ç½®** (r=320)ï¼šä½¿ç”¨é è¨“ç·´ LoRA æ¬Šé‡ï¼Œæ”¶æ–‚æ›´å¿«
2. **è«–æ–‡é…ç½®** (r=64)ï¼šå¾é›¶è¨“ç·´ï¼Œåš´æ ¼å¾©ç¾è«–æ–‡è¦æ ¼

## é…ç½®å°ç…§

| é …ç›® | é è¨“ç·´é…ç½® (r=320) | è«–æ–‡é…ç½® (r=64) |
|------|-------------------|----------------|
| Speech LoRA rank | 320 | 64 â­ |
| Speech LoRA alpha | 640 | 128 â­ |
| Speech dropout | 0.01 | 0.05 â­ |
| Vision LoRA rank | 256 | 256 |
| Vision LoRA alpha | 512 | 512 |
| å¯è¨“ç·´åƒæ•¸ | 830M (14.9%) | ~200M (3.5%) |
| è¨“ç·´èµ·é» | é è¨“ç·´ LoRA æ¬Šé‡ | éš¨æ©Ÿåˆå§‹åŒ– â­ |
| è¼¸å‡ºç›®éŒ„ | output/pretrained_r320/ | output/paper_r64/ |
| é æœŸæ”¶æ–‚é€Ÿåº¦ | è¼ƒå¿« | è¼ƒæ…¢ï¼ˆéœ€æ›´å¤š epochï¼‰ |
| è«–æ–‡ç¬¦åˆåº¦ | éƒ¨åˆ† | å®Œå…¨ç¬¦åˆ â­ |

â­ = è«–æ–‡åŸå§‹è¦æ ¼

## è¨“ç·´æ–¹å¼

### æ–¹å¼ 1ï¼šäº¤äº’å¼è¨“ç·´ï¼ˆè¨“ç·´å…©ç¨®é…ç½®ï¼‰

```bash
source run_env.sh
cd src
python train_dual_configs.py
```

**äº¤äº’å¼é¸é …**ï¼š
1. å…ˆè¨“ç·´é è¨“ç·´é…ç½®ï¼Œå†è¨“ç·´è«–æ–‡é…ç½®
2. å…ˆè¨“ç·´è«–æ–‡é…ç½®ï¼Œå†è¨“ç·´é è¨“ç·´é…ç½®
3. åƒ…è¨“ç·´é è¨“ç·´é…ç½®
4. åƒ…è¨“ç·´è«–æ–‡é…ç½®

### æ–¹å¼ 2ï¼šå‘½ä»¤è¡Œè¨“ç·´ï¼ˆå–®ä¸€é…ç½®ï¼‰

**è¨“ç·´é è¨“ç·´é…ç½®**ï¼š
```bash
source run_env.sh
cd src
python train_single_config.py --config pretrained_r320
```

**è¨“ç·´è«–æ–‡é…ç½®**ï¼š
```bash
source run_env.sh
cd src
python train_single_config.py --config paper_r64
```

**è‡ªå®šç¾©åƒæ•¸**ï¼š
```bash
python train_single_config.py \
    --config paper_r64 \
    --epochs 4 \
    --batch-size 4 \
    --gradient-accumulation 16 \
    --learning-rate 2e-5
```

## è¨“ç·´åƒæ•¸ï¼ˆè«–æ–‡è¦æ ¼ï¼‰

æ ¹æ“šè«–æ–‡ Table 3 çš„æœ€ä½³é…ç½®ï¼š

```python
num_train_epochs = 3                    # è«–æ–‡æœ€ä½³çµæœåœ¨ epoch 3
per_device_train_batch_size = 8         # è«–æ–‡è¨­å®š
gradient_accumulation_steps = 8         # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 64
learning_rate = 2e-5                    # è«–æ–‡è¨­å®šï¼ˆ2Ã—10â»âµï¼‰
optimizer = "adamw_torch"               # Adam å„ªåŒ–å™¨
bf16 = True                             # bfloat16 ç²¾åº¦
max_length = 2048                       # å®¹ç´éŸ³è¨Š token (SFTConfig ä½¿ç”¨ max_length)
```

## è¼¸å‡ºçµæ§‹

è¨“ç·´å¾Œçš„ç›®éŒ„çµæ§‹ï¼š

```
output/
â”œâ”€â”€ pretrained_r320/
â”‚   â”œâ”€â”€ checkpoint-epoch-1/
â”‚   â”œâ”€â”€ checkpoint-epoch-2/
â”‚   â”œâ”€â”€ checkpoint-epoch-3/
â”‚   â”œâ”€â”€ final_model/
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ events.out.tfevents.*
â”‚   â””â”€â”€ training_config.json
â”‚
â””â”€â”€ paper_r64/
    â”œâ”€â”€ checkpoint-epoch-1/
    â”œâ”€â”€ checkpoint-epoch-2/
    â”œâ”€â”€ checkpoint-epoch-3/
    â”œâ”€â”€ final_model/
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â”œâ”€â”€ adapter_model.safetensors
    â”‚   â”œâ”€â”€ tokenizer_config.json
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ events.out.tfevents.*
    â””â”€â”€ training_config.json
```

## ç›£æ§è¨“ç·´

### TensorBoard

**æŸ¥çœ‹é è¨“ç·´é…ç½®çš„è¨“ç·´éç¨‹**ï¼š
```bash
tensorboard --logdir output/pretrained_r320/logs
```

**æŸ¥çœ‹è«–æ–‡é…ç½®çš„è¨“ç·´éç¨‹**ï¼š
```bash
tensorboard --logdir output/paper_r64/logs
```

**åŒæ™‚æŸ¥çœ‹å…©ç¨®é…ç½®**ï¼š
```bash
tensorboard --logdir output/
```

ç„¶å¾Œåœ¨ç€è¦½å™¨ä¸­æ‰“é–‹ http://localhost:6006

### è¨“ç·´é…ç½®æª”æ¡ˆ

æ¯å€‹è¨“ç·´é‹è¡Œéƒ½æœƒç”Ÿæˆ `training_config.json`ï¼ŒåŒ…å«ï¼š
- é…ç½®åç¨±å’Œæè¿°
- Speech LoRA å’Œ Vision LoRA åƒæ•¸
- è¨“ç·´è¶…åƒæ•¸
- å¯è¨“ç·´åƒæ•¸çµ±è¨ˆ

ç¤ºä¾‹ï¼š
```json
{
  "config_name": "paper_r64",
  "description": "è«–æ–‡ LoRA é…ç½®ï¼ˆr=64ï¼‰ï¼ŒSpeech LoRA å¾é›¶è¨“ç·´",
  "speech_lora": {"r": 64, "alpha": 128, "dp": 0.05},
  "vision_lora": {"r": 256, "alpha": 512, "dp": 0.0},
  "trainable_params": "~200M (3.5%)",
  "training_args": {
    "num_train_epochs": 3,
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 8,
    "learning_rate": 2e-05,
    "effective_batch_size": 64
  }
}
```

## ç¡¬é«”éœ€æ±‚

### VRAM éœ€æ±‚
- **bfloat16 ç„¡é‡åŒ–**ï¼šç´„ 40-45GB
- **å»ºè­°ç¡¬é«”**ï¼š
  - NVIDIA A100 (80GB)
  - NVIDIA A6000 (48GB)
  - å¤šå¼µ RTX 4090 (24GB Ã— 2)

### è¨“ç·´æ™‚é–“ä¼°è¨ˆ
- **é è¨“ç·´é…ç½® (r=320)**ï¼šç´„ 6-8 å°æ™‚ / epochï¼ˆé è¨“ç·´ LoRAï¼Œæ”¶æ–‚å¿«ï¼‰
- **è«–æ–‡é…ç½® (r=64)**ï¼šç´„ 4-6 å°æ™‚ / epochï¼ˆåƒæ•¸è¼ƒå°‘ï¼Œä½†å¾é›¶è¨“ç·´ï¼‰
- **ç¸½æ™‚é–“ï¼ˆ3 epochsï¼‰**ï¼šç´„ 12-24 å°æ™‚

## è©•ä¼°è¨“ç·´çµæœ

è¨“ç·´å®Œæˆå¾Œï¼Œä½¿ç”¨ `src/estimate.py` è©•ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```bash
# è©•ä¼°é è¨“ç·´é…ç½®
python estimate.py \
    --model-path ../output/pretrained_r320/final_model \
    --test-data ../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/test/

# è©•ä¼°è«–æ–‡é…ç½®
python estimate.py \
    --model-path ../output/paper_r64/final_model \
    --test-data ../../DataSets/Reproduce_English_Pronunciation/speechocean762_formatted/test/
```

### è«–æ–‡åŸºæº–æ€§èƒ½ï¼ˆPaper Table 3, LoRA-only, Epoch 3ï¼‰

| æŒ‡æ¨™ | ç›®æ¨™å€¼ |
|------|--------|
| Accuracy PCC | 0.656 |
| Fluency PCC | 0.727 |
| Prosodic PCC | 0.711 |
| Total PCC | 0.675 |
| WER | 0.140 |
| PER | 0.114 |
| F1-score | 0.724 |

## é…ç½®é¸æ“‡å»ºè­°

### é¸æ“‡é è¨“ç·´é…ç½® (r=320) å¦‚æœï¼š
- âœ… éœ€è¦å¿«é€Ÿé©—è­‰è¨“ç·´æµç¨‹
- âœ… æƒ³è¦æ›´å¿«çš„æ”¶æ–‚é€Ÿåº¦
- âœ… å°ˆæ¡ˆç›®æ¨™æ˜¯å¯¦ç”¨æ€§ï¼Œä¸éœ€è¦åš´æ ¼å¾©ç¾è«–æ–‡
- âœ… æœ‰è¶³å¤ çš„ VRAMï¼ˆ830M å¯è¨“ç·´åƒæ•¸ï¼‰

### é¸æ“‡è«–æ–‡é…ç½® (r=64) å¦‚æœï¼š
- âœ… éœ€è¦åš´æ ¼å¾©ç¾è«–æ–‡çµæœ
- âœ… æƒ³è¦èˆ‡è«–æ–‡åŸºæº–é€²è¡Œå…¬å¹³æ¯”è¼ƒ
- âœ… ç ”ç©¶ç›®æ¨™æ˜¯é©—è­‰è«–æ–‡æ–¹æ³•è«–
- âœ… å¯ä»¥æ¥å—è¼ƒæ…¢çš„æ”¶æ–‚é€Ÿåº¦å’Œæ›´å¤šè¨“ç·´æ™‚é–“

### å»ºè­°ï¼šè¨“ç·´å…©ç¨®é…ç½®
- å…ˆè¨“ç·´**è«–æ–‡é…ç½®**é©—è­‰è«–æ–‡å¾©ç¾èƒ½åŠ›
- å†è¨“ç·´**é è¨“ç·´é…ç½®**æ¢ç´¢æ€§èƒ½ä¸Šé™
- æ¯”è¼ƒå…©ç¨®é…ç½®çš„æ€§èƒ½å·®ç•°ï¼Œåˆ†æé è¨“ç·´ LoRA çš„åƒ¹å€¼

## å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼è«–æ–‡é…ç½®åƒæ•¸æ›´å°‘ä½†è¨“ç·´æ™‚é–“å¯èƒ½æ›´é•·ï¼Ÿ
A: è«–æ–‡é…ç½®çš„ LoRA å¾é›¶é–‹å§‹éš¨æ©Ÿåˆå§‹åŒ–ï¼Œéœ€è¦æ›´å¤šè¨“ç·´æ­¥é©Ÿæ‰èƒ½æ”¶æ–‚ã€‚é è¨“ç·´é…ç½®å¾å·²ç¶“è¨“ç·´å¥½çš„ LoRA æ¬Šé‡é–‹å§‹ï¼Œå¯ä»¥æ›´å¿«é”åˆ°è‰¯å¥½æ€§èƒ½ã€‚

### Q: å…©ç¨®é…ç½®å¯ä»¥åŒæ™‚è¨“ç·´å—ï¼Ÿ
A: ä¸å»ºè­°ã€‚æ¯å€‹é…ç½®éœ€è¦ç´„ 40-45GB VRAMï¼ŒåŒæ™‚è¨“ç·´éœ€è¦ 80-90GB VRAMã€‚å»ºè­°ä¾åºè¨“ç·´ã€‚

### Q: å¦‚ä½•é¸æ“‡è¨“ç·´é †åºï¼Ÿ
A: å»ºè­°å…ˆè¨“ç·´**è«–æ–‡é…ç½®** (r=64)ï¼Œå› ç‚ºï¼š
1. é©—è­‰è«–æ–‡å¾©ç¾èƒ½åŠ›
2. è¨“ç·´æ™‚é–“è¼ƒçŸ­ï¼ˆåƒæ•¸è¼ƒå°‘ï¼‰
3. å¯ä»¥ä½œç‚ºåŸºæº–èˆ‡é è¨“ç·´é…ç½®æ¯”è¼ƒ

### Q: è¨“ç·´å¤±æ•—å¦‚ä½•æ¢å¾©ï¼Ÿ
A: è¨“ç·´æœƒåœ¨æ¯å€‹ epoch çµæŸæ™‚ä¿å­˜ checkpointï¼Œå¯ä»¥å¾æœ€è¿‘çš„ checkpoint ç¹¼çºŒè¨“ç·´ã€‚ä¿®æ”¹è¨“ç·´è…³æœ¬ä½¿ç”¨ `trainer.train(resume_from_checkpoint="output/xxx/checkpoint-epoch-N")` æ¢å¾©ã€‚

### Q: å¦‚ä½•èª¿æ•´è¨“ç·´åƒæ•¸ï¼Ÿ
A: ä½¿ç”¨ `train_single_config.py` çš„å‘½ä»¤è¡Œåƒæ•¸èª¿æ•´ï¼š
```bash
python train_single_config.py \
    --config paper_r64 \
    --epochs 4 \              # å¢åŠ è¨“ç·´è¼ªæ•¸
    --batch-size 4 \          # æ¸›å°‘æ‰¹æ¬¡å¤§å°ï¼ˆVRAM ä¸è¶³æ™‚ï¼‰
    --gradient-accumulation 16 \  # å¢åŠ æ¢¯åº¦ç´¯ç©ï¼ˆä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼‰
    --learning-rate 1e-5      # èª¿æ•´å­¸ç¿’ç‡
```

## å¯¦ä½œç´°ç¯€

### é—œéµæª”æ¡ˆ

1. **[src/model_utility_configs.py](../src/model_utility_configs.py)**
   - å…©ç¨®é…ç½®çš„æ¨¡å‹è¼‰å…¥å‡½æ•¸
   - é…ç½®å°ç…§è¡¨ `CONFIGS` å­—å…¸

2. **[src/train_dual_configs.py](../src/train_dual_configs.py)**
   - äº¤äº’å¼é›™é…ç½®è¨“ç·´è…³æœ¬
   - æ”¯æ´é¸æ“‡è¨“ç·´é †åº

3. **[src/train_single_config.py](../src/train_single_config.py)**
   - å‘½ä»¤è¡Œå–®ä¸€é…ç½®è¨“ç·´è…³æœ¬
   - æ”¯æ´è‡ªå®šç¾©è¨“ç·´åƒæ•¸

4. **[src/AudioDataCollator.py](../src/AudioDataCollator.py)**
   - éŸ³è¨Šæ•¸æ“šæ‰¹æ¬¡è™•ç†å™¨
   - è™•ç†å¡«å……å’Œæ¨™ç±¤é®ç½©

5. **[src/data_utility.py](../src/data_utility.py)**
   - SpeechOcean762 æ•¸æ“šé›†æ ¼å¼åŒ–
   - æ”¯æ´ TorchCodec AudioDecoder

### æ ¸å¿ƒå·®ç•°

**é è¨“ç·´é…ç½®è¼‰å…¥**ï¼ˆmodel_utility_configs.py:78-98ï¼‰ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,  # r=320 é…ç½®
    torch_dtype=torch.bfloat16,
    # ä¸ä½¿ç”¨ ignore_mismatched_sizes
)
# LoRA æ¬Šé‡å¾ checkpoint è¼‰å…¥ï¼ˆr=320ï¼‰
```

**è«–æ–‡é…ç½®è¼‰å…¥**ï¼ˆmodel_utility_configs.py:165-174ï¼‰ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,  # r=64 é…ç½®
    torch_dtype=torch.bfloat16,
    ignore_mismatched_sizes=True,  # ğŸ”‘ é—œéµ
)
# LoRA æ¬Šé‡è¢«é‡æ–°åˆå§‹åŒ–ï¼ˆr=64ï¼‰
```

### PEFT è£œä¸

å…©ç¨®é…ç½®éƒ½ä½¿ç”¨ç›¸åŒçš„ PEFT è£œä¸ï¼ˆmodel_utility_configs.py:14-30ï¼‰ï¼š
```python
def _patched_peft_init(self, model, peft_config, adapter_name="default", **kwargs):
    if not hasattr(model, 'prepare_inputs_for_generation'):
        def prepare_inputs_for_generation(*args, **kwargs):
            return {}
        model.prepare_inputs_for_generation = prepare_inputs_for_generation
    _original_peft_init(self, model, peft_config, adapter_name, **kwargs)
```

é€™è§£æ±ºäº† Phi-4-multimodal çš„æ¶æ§‹ä¸å…¼å®¹å•é¡Œã€‚

## åƒè€ƒè³‡æ–™

- [PEFT/LoRA ä¸å…¼å®¹å•é¡Œæ–‡æª”](peft_lora_incompatibility.md)
- [å¾é›¶è¨“ç·´ LoRA é…ç½®æŒ‡å—](lora_from_scratch_config.md)
- [è«–æ–‡åŸæ–‡](../paper/)
- [å°ˆæ¡ˆ CLAUDE.md](../CLAUDE.md)
