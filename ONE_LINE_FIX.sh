#!/bin/bash
# ONE-LINE FIX FOR REMOTE MACHINE
# Copy each command block to your remote terminal

cat << 'EOF'
================================================================================
ONE-LINE FIX FOR REMOTE MACHINE
================================================================================

STEP 1: Stop Current Training
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
pkill -f train_single_config_remote.py


STEP 2: Go to Project Directory
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
cd /datas/store162/xrick/prjs/Reproduce_English_Pronunciation_Evaluation


STEP 3: Apply LoRA Fix (CRITICAL)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
sed -i.backup '/task_type="CAUSAL_LM",$/a\
\
    # CRITICAL: Apply LoRA configuration to enable training\
    print("\\nðŸ”§ Applying LoRA configuration to model...")\
    model = get_peft_model(model, peft_config)\
    print("âœ… LoRA configuration applied - parameters are now trainable")' src/model_utility_configs.py


STEP 4: Fix FP16
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
sed -i 's/torch\.bfloat16/torch.float16/g' src/model_utility_configs.py


STEP 5: Disable AMP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
sed -i 's/"fp16": use_fp16,/"fp16": False,  # Disabled AMP/' src/train_single_config_remote.py


STEP 6: Verify All Fixes
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo ""
echo "=== LoRA Fix Check ==="
grep -A 2 "get_peft_model" src/model_utility_configs.py && echo "âœ… LoRA fix applied" || echo "âŒ LoRA fix MISSING"

echo ""
echo "=== FP16 Fix Check ==="
grep "torch.float16" src/model_utility_configs.py | head -1 && echo "âœ… FP16 fix applied" || echo "âŒ FP16 fix MISSING"

echo ""
echo "=== AMP Fix Check ==="
grep '"fp16": False' src/train_single_config_remote.py && echo "âœ… AMP fix applied" || echo "âŒ AMP fix MISSING"


STEP 7: Clean and Restart
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
rm -rf src/output/paper_r64/
source venv/bin/activate
python src/train_single_config_remote.py --config paper_r64 --gpus 0


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EXPECTED OUTPUT (Model Loading):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ”§ Applying LoRA configuration to model...
âœ… LoRA configuration applied - parameters are now trainable
å¯è¨“ç·´åƒæ•¸: 200,000,000 (3.5%)  â† MUST BE ~200M, NOT 0!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EXPECTED OUTPUT (Training):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{'loss': 6.98, 'epoch': 0.26}  â† Starts around 7.0
{'loss': 6.42, 'epoch': 0.51}  â† Decreases
{'loss': 5.89, 'epoch': 0.77}  â† Continues down

NO WARNING about "requires_grad"!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EOF
